{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1904163b",
   "metadata": {},
   "source": [
    "Hosting an LLM for your organization, community, or even for yourself requires a lot of critical thinking alongside optimization steps, because the cost will affect your business approach. Companies like Together.ai or Nebius AI, which provide open-source LLMs as APIs, have implemented many optimization techniques to reduce the cost of hosting an LLM.\n",
    "\n",
    "These optimization techniques are distributed across three different phases:\n",
    "\n",
    "1. **Model Compression & Quantization Techniques**\n",
    "   - **Weight-Only Quantization:** Lowers weight precision (e.g., 4-bit) for speed and memory savings.\n",
    "   - **AWQ:** Advanced quantization that preserves accuracy.\n",
    "   - **KV Cache Quantization:** Reduces KV cache memory with int8/int4 quantization.\n",
    "   - **Combined Techniques:** Mixes AWQ and KV cache quantization for best results.\n",
    "\n",
    "2. **Core Inference Optimization Techniques**\n",
    "   - **Persistent Batching:** Continuously adds requests to a running batch for better GPU use and lower latency.\n",
    "   - **Blocked KV Cache (Paged Attention):** Uses non-contiguous memory blocks for KV cache to reduce fragmentation and share memory efficiently.\n",
    "   - **High-Performance CUDA Kernels:** Custom GPU code for faster operations.\n",
    "   - **Tensor Parallelism:** Splits model across multiple GPUs for large models.\n",
    "   - **FlashAttention / Flash-Decoding:** Fast attention algorithms for long sequences.\n",
    "   - **CUDA Graphs:** Captures GPU operations to reduce CPU overhead and latency.\n",
    "   - **GQA Optimization:** Speeds up Grouped-Query Attention models.\n",
    "   - **Long Context Handling:** Efficiently manages long inputs with NTK-RoPE scaling, LogN scaling, and prefix caching.\n",
    "\n",
    "3. **Deployment & Serving Features**\n",
    "   - **RESTful API Server:** Standard HTTP interface for model access.\n",
    "   - **Multi-Model Serving:** Hosts several models in one deployment.\n",
    "   - **Distributed Serving:** Spreads requests across machines/GPUs for scaling.\n",
    "   - **Vision-Language Model Support:** Handles text and image inputs efficiently.\n",
    "   - **Function Calling Support:** Enables structured outputs for tool/API integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80175998",
   "metadata": {},
   "source": [
    "# Understanding  Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07b556",
   "metadata": {},
   "source": [
    "If you dont have access to HuggingFace Model Hub (Run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86300887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook login huggingface.co\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d944e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model ID (We are using Llama-3.2 1B model)\n",
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8f5ec",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from Hugging Face Transformers for model and tokenizer handling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# Import PyTorch for tensor operations\n",
    "import torch\n",
    "# Import time for measuring performance\n",
    "import time\n",
    "# Import tqdm for progress bars in Jupyter notebooks\n",
    "from tqdm.notebook import tqdm\n",
    "# Import json for handling JSON data\n",
    "import json\n",
    "\n",
    "# Import the custom function for evaluating model outputs using an LLM judge\n",
    "from llm_as_eval import evaluate_with_judge\n",
    "\n",
    "# Import the warnings module to suppress unnecessary warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LLM_API_KEY = \"YOUR_LLM_API_KEY\"  # Replace with your actual API key (OpenAI, HuggingFace, Nebius, Together AI etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2ffdf",
   "metadata": {},
   "source": [
    "Loading original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2efa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for the specified model\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the causal language model with bfloat16 precision on CPU\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\" # Use \"auto\" for automatic device mapping if you have a GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4330007",
   "metadata": {},
   "source": [
    "Create a function to generate text, calculate memory usage, and measure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a9962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text from a model and tokenizer with optional memory and time measurement\n",
    "def generate_text(tokenizer, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate text using a given tokenizer and model.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer for encoding input text.\n",
    "        model: The language model for text generation.\n",
    "        **kwargs: Additional arguments for model.generate(), including:\n",
    "            - input_text (str): The prompt to generate from.\n",
    "            - max_new_tokens, do_sample, temperature, top_p, top_k, pad_token_id, etc.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated text (str), peak memory usage in MB (float or None),\n",
    "               and generation time in seconds (float).\n",
    "    \"\"\"\n",
    "    # Extract the input text from keyword arguments\n",
    "    input_text = kwargs.pop('input_text')\n",
    "    # Tokenize the input text and convert it to PyTorch tensors\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Check if a CUDA-enabled GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        # Reset peak memory statistics for the current CUDA device\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        # Get the device of the model (e.g., 'cuda:0')\n",
    "        device = next(model.parameters()).device\n",
    "        # Move the input tensors to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Record the start time for generation\n",
    "    start_time = time.time()\n",
    "    # Generate text using the model with the provided inputs and generation arguments\n",
    "    output_tokens = model.generate(**inputs, **kwargs)\n",
    "    # Record the end time for generation\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the total generation time and round it to 3 decimal places\n",
    "    generation_time = round(end_time - start_time, 3)\n",
    "    \n",
    "    # Check if a CUDA-enabled GPU is available to measure memory\n",
    "    if torch.cuda.is_available():\n",
    "        # Measure the peak memory allocated on the GPU during generation\n",
    "        # Convert bytes to megabytes (MB) and round to 3 decimal places\n",
    "        peak_memory = round(torch.cuda.max_memory_allocated() / (1024 ** 2), 3)\n",
    "    else:\n",
    "        # If no GPU is available, set peak memory to None\n",
    "        peak_memory = None\n",
    "        \n",
    "    # Decode the generated tokens back into a string, skipping special tokens\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    # Return the generated text, peak memory usage, and generation time\n",
    "    return generated_text, peak_memory, generation_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102c921",
   "metadata": {},
   "source": [
    "Create a function to measure memory footprint of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f126e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_footprint(model):\n",
    "    \"\"\"\n",
    "    Get the memory footprint of a model in megabytes.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to measure memory footprint for.\n",
    "    \n",
    "    Returns:\n",
    "        float: Memory footprint in megabytes, rounded to 2 decimal places.\n",
    "    \"\"\"\n",
    "    # Get memory footprint in bytes and convert to megabytes\n",
    "    memory_bytes = model.get_memory_footprint()\n",
    "    memory_mb = memory_bytes / (1024 * 1024)\n",
    "    \n",
    "    return round(memory_mb, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14400da7",
   "metadata": {},
   "source": [
    "Running a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a568c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Some people are equal, but some people are more equal than others\n",
      "Model Memory Footprint (MB): 2357.13\n",
      "Peak Memory Usage (MB): 1240.101\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate text using the loaded model and tokenizer\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"Some people are equal, but some people are more\"\n",
    "\n",
    "# Generate response with specified generation parameters\n",
    "generated_response = generate_text(\n",
    "    input_text=prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    model=original_model,\n",
    "    max_new_tokens=3,           # Limit to 3 new tokens\n",
    "    do_sample=True,             # Enable sampling\n",
    "    temperature=0.7,            # Sampling temperature\n",
    "    top_p=0.9,                  # Nucleus sampling\n",
    "    top_k=50,                   # Top-k sampling\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Padding token\n",
    ")\n",
    "\n",
    "# Print the generated response and peak memory usage\n",
    "print(\"Generated Response:\", generated_response[0])\n",
    "\n",
    "# Print the model's memory footprint\n",
    "model_memory = get_model_memory_footprint(original_model)\n",
    "print(\"Model Memory Footprint (MB):\", model_memory)\n",
    "\n",
    "if generated_response[1] is not None:\n",
    "    print(\"Peak Memory Usage (MB):\", generated_response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb430ea",
   "metadata": {},
   "source": [
    "Looking at the architecture of llama 3.2 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce41a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49aaec",
   "metadata": {},
   "source": [
    "(Original Model) Let's look at the weights of the query projection (q_proj) in the self-attention mechanism of the first decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe6bc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
       "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
       "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
       "        ...,\n",
       "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
       "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
       "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the weight matrix of the query projection (q_proj) \n",
    "# in the self-attention mechanism of the first decoder layer of the model.\n",
    "q_proj_weight = original_model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "# Display the weight tensor\n",
    "q_proj_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff69f44",
   "metadata": {},
   "source": [
    "Loading our eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3f24fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question 1: What color is the sky?\n",
      "Sample Answer 1: The sky is blue.\n",
      "\n",
      "Sample Question 2: How many legs does a cat have?\n",
      "Sample Answer 2: It has four legs.\n",
      "Total Number of Questions: 49\n"
     ]
    }
   ],
   "source": [
    "# Import the json library to work with JSON files\n",
    "import json\n",
    "\n",
    "# --- Load Evaluation Dataset ---\n",
    "# Open the evaluation data file in read mode\n",
    "with open('eval_data.json', 'r') as file:\n",
    "    # Load the JSON content from the file into the 'eval_data' variable\n",
    "    eval_data = json.load(file)\n",
    "\n",
    "# --- Display Sample Questions and Answers ---\n",
    "# Print the first two questions and their answers from the dataset to verify it's loaded correctly\n",
    "print(\"Sample Question 1:\", eval_data[0]['q'])\n",
    "print(\"Sample Answer 1:\", eval_data[0]['a'])\n",
    "print(\"\\nSample Question 2:\", eval_data[1]['q'])\n",
    "print(\"Sample Answer 2:\", eval_data[1]['a'])\n",
    "\n",
    "# --- Total Number of Questions ---\n",
    "# Print the total number of questions in the evaluation dataset\n",
    "print(\"Total Number of Questions:\", len(eval_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237b32e",
   "metadata": {},
   "source": [
    "Let's create a function for evaluating the model on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da13ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate the Model ---\n",
    "def evaluate_model(model, tokenizer, eval_data, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to evaluate.\n",
    "        tokenizer: The tokenizer for encoding input text.\n",
    "        eval_data: A list of dictionaries containing evaluation data with 'q' (question) and 'a' (answer).\n",
    "        **kwargs: Additional arguments to be passed to the generate_text function.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the question, \n",
    "              generated answer, ground truth answer, peak memory usage (MB), \n",
    "              and generation time (seconds).\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Set default generation parameters\n",
    "    generation_params = {\n",
    "        'max_new_tokens': 10,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 50,\n",
    "    }\n",
    "    \n",
    "    # Update with any user-provided kwargs\n",
    "    generation_params.update(kwargs)\n",
    "    \n",
    "    for item in tqdm(eval_data, desc=\"Evaluating model\"):\n",
    "        question = item['q']\n",
    "        ground_truth_answer = item['a']\n",
    "        \n",
    "        # Generate an answer using the model and capture all metrics\n",
    "        generated_answer, peak_memory, generation_time = generate_text(\n",
    "            input_text=question,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            **generation_params\n",
    "        )\n",
    "        \n",
    "        # Append the results as a dictionary\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"peak_memory_mb\": peak_memory,\n",
    "            \"generation_time_s\": generation_time\n",
    "        })\n",
    "\n",
    "    # Return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c20cf62",
   "metadata": {},
   "source": [
    "# Evaluation of Base Model\n",
    "### (Act as a Baseline for Latency, Accuracy, Memory) Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd23f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08b290a17914f4eb8274a71278fd61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the evaluation on the model with the evaluation dataset\n",
    "base_model_results = evaluate_model(original_model, tokenizer, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc80cc7",
   "metadata": {},
   "source": [
    "In case you want to save the evaluation results to a JSON file, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb506be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Base Model Results to a JSON File ---\n",
    "# This is useful for saving the results of a long evaluation so you don't have to run it again.\n",
    "with open('base_model_results.json', 'w') as f:\n",
    "    # Use json.dump to write the base_model_results list to the file\n",
    "    # indent=4 makes the JSON file human-readable\n",
    "    json.dump(base_model_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c2c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Base Model Results from the JSON File ---\n",
    "# This demonstrates how to load the results back, for example, in a different session.\n",
    "with open('base_model_results.json', 'r') as f:\n",
    "    # Load the JSON data from the file back into the base_model_results variable\n",
    "    base_model_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c94604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:06<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate with LLM Judge ---\n",
    "# Use the 'evaluate_with_judge' function to assess the model's performance.\n",
    "# This function takes the model's results and an API key for the judging service.\n",
    "# It returns a DataFrame with detsailed results and a dictionary of overall metrics.\n",
    "# Note: You need to replace \"your_nebius_api_key_here\" with your actual Nebius API key.\n",
    "base_model_results_df, base_model_metrics = evaluate_with_judge(\n",
    "    base_model_results,  # The results from your model evaluation\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6e0f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>peak_memory_mb</th>\n",
       "      <th>generation_time_s</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>llm_judge_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What color is the sky?</td>\n",
       "      <td>What color is the sky? What color is the ocean...</td>\n",
       "      <td>The sky is blue.</td>\n",
       "      <td>1240.194</td>\n",
       "      <td>3.912</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many legs does a cat have?</td>\n",
       "      <td>How many legs does a cat have? The answer depe...</td>\n",
       "      <td>It has four legs.</td>\n",
       "      <td>1240.257</td>\n",
       "      <td>3.976</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What sound does a dog make?</td>\n",
       "      <td>What sound does a dog make? What sound does a ...</td>\n",
       "      <td>A dog says woof.</td>\n",
       "      <td>1240.226</td>\n",
       "      <td>4.084</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many days in a week?</td>\n",
       "      <td>How many days in a week? How many days in a mo...</td>\n",
       "      <td>There are seven days.</td>\n",
       "      <td>1240.226</td>\n",
       "      <td>3.974</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What color is grass?</td>\n",
       "      <td>What color is grass? It can be green, brown, o...</td>\n",
       "      <td>Grass is green.</td>\n",
       "      <td>1240.163</td>\n",
       "      <td>3.945</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0          What color is the sky?   \n",
       "1  How many legs does a cat have?   \n",
       "2     What sound does a dog make?   \n",
       "3        How many days in a week?   \n",
       "4            What color is grass?   \n",
       "\n",
       "                                    generated_answer    ground_truth_answer  \\\n",
       "0  What color is the sky? What color is the ocean...       The sky is blue.   \n",
       "1  How many legs does a cat have? The answer depe...      It has four legs.   \n",
       "2  What sound does a dog make? What sound does a ...       A dog says woof.   \n",
       "3  How many days in a week? How many days in a mo...  There are seven days.   \n",
       "4  What color is grass? It can be green, brown, o...        Grass is green.   \n",
       "\n",
       "   peak_memory_mb  generation_time_s  similarity_score llm_judge_response  \n",
       "0        1240.194              3.912               0.2                0.2  \n",
       "1        1240.257              3.976               0.2                0.2  \n",
       "2        1240.226              4.084               0.2                0.2  \n",
       "3        1240.226              3.974               0.2                0.2  \n",
       "4        1240.163              3.945               0.6                0.6  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_results_df.head()  # Display the first few rows of the DataFrame with evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6a1417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Performance Metrics:\n",
      "- Avg Latency: 4.4598\n",
      "- Avg Memory: 1240.2071\n",
      "- Avg Score: 0.3939\n"
     ]
    }
   ],
   "source": [
    "# --- Display Base Model Metrics ---\n",
    "# Print the overall performance metrics for the base model.\n",
    "# These metrics include average latency, memory usage, and similarity score.\n",
    "print(\"Base Model Performance Metrics:\")\n",
    "for key, value in base_model_metrics.items():\n",
    "    # Print each metric with its corresponding value, formatted to 4 decimal places\n",
    "    print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4741094",
   "metadata": {},
   "source": [
    "# Weight only Quantization\n",
    "\n",
    "Reduces the precision of the model's weights (e.g., from 16-bit floats to 4-bit integers) while keeping activations in higher precision (e.g., 16-bit float). This is a popular trade-off for speed vs. accuracy.\n",
    "\n",
    "| Format    | Meaning                                 |\n",
    "| --------- | --------------------------------------- |\n",
    "| **W4A16** | Weights in 4-bit, Activations in 16-bit |\n",
    "| **W8A8**  | Weights and Activations both in 8-bit   |\n",
    "| **W4A8**  | Weights in 4-bit, Activations in 8-bit  |\n",
    "| **W8A16** | Weights in 8-bit, Activations in 16-bit |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed535dc",
   "metadata": {},
   "source": [
    "### For W4A16 Quantized Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define quantization config: 4-bit weights (W4A16)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,          # optional: improves accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",               # \"nf4\" (normal float 4-bit) or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16    # use bfloat16 for activations (W4A16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e902d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization (weights only)\n",
    "w4a16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"  # use \"auto\" for best device placement (GPU/CPU)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa0ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Memory Footprint (MB): 965.13\n"
     ]
    }
   ],
   "source": [
    "# Memory footprint of the quantized model\n",
    "w4a16_model_memory = get_model_memory_footprint(w4a16_model)\n",
    "print(\"Quantized Model Memory Footprint (MB):\", w4a16_model_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1af3d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aca24ef16c4046b7bf6de24e878d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Evaluate the W4A16 Quantized Model ---\n",
    "# Running the evaluation on the quantized model with the evaluation dataset\n",
    "w4a16_model_results = evaluate_model(w4a16_model, tokenizer, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c494423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:08<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate with LLM Judge ---\n",
    "# Use the 'evaluate_with_judge' function to assess the quantized model's performance.\n",
    "w4a16_model_results_df, w4a16_model_metrics = evaluate_with_judge(\n",
    "    w4a16_model_results,  # The results from your quantized model evaluation\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14453ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>peak_memory_mb</th>\n",
       "      <th>generation_time_s</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>llm_judge_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What color is the sky?</td>\n",
       "      <td>What color is the sky? What color is the sky? ...</td>\n",
       "      <td>The sky is blue.</td>\n",
       "      <td>1053.762</td>\n",
       "      <td>2.781</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many legs does a cat have?</td>\n",
       "      <td>How many legs does a cat have? (I know, you're...</td>\n",
       "      <td>It has four legs.</td>\n",
       "      <td>1053.919</td>\n",
       "      <td>1.367</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What sound does a dog make? woof or howl?</td>\n",
       "      <td>What sound does a dog make? woof or howl? What...</td>\n",
       "      <td>A dog says woof.</td>\n",
       "      <td>1054.311</td>\n",
       "      <td>1.416</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many days in a week?</td>\n",
       "      <td>How many days in a week? 3.5? 5? 7</td>\n",
       "      <td>There are seven days.</td>\n",
       "      <td>1053.840</td>\n",
       "      <td>1.380</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What color is grass?</td>\n",
       "      <td>What color is grass? (2015)\\n1 What color is g...</td>\n",
       "      <td>Grass is green.</td>\n",
       "      <td>1053.684</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question  \\\n",
       "0                     What color is the sky?   \n",
       "1             How many legs does a cat have?   \n",
       "2  What sound does a dog make? woof or howl?   \n",
       "3                   How many days in a week?   \n",
       "4                       What color is grass?   \n",
       "\n",
       "                                    generated_answer    ground_truth_answer  \\\n",
       "0  What color is the sky? What color is the sky? ...       The sky is blue.   \n",
       "1  How many legs does a cat have? (I know, you're...      It has four legs.   \n",
       "2  What sound does a dog make? woof or howl? What...       A dog says woof.   \n",
       "3                 How many days in a week? 3.5? 5? 7  There are seven days.   \n",
       "4  What color is grass? (2015)\\n1 What color is g...        Grass is green.   \n",
       "\n",
       "   peak_memory_mb  generation_time_s  similarity_score llm_judge_response  \n",
       "0        1053.762              2.781               0.2                0.2  \n",
       "1        1053.919              1.367               0.1                0.1  \n",
       "2        1054.311              1.416               0.8                0.8  \n",
       "3        1053.840              1.380               0.6                0.6  \n",
       "4        1053.684              1.389               0.0                0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w4a16_model_results_df.head()  # Display the first few rows of the DataFrame with evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af44bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A16 Quantized Model Performance Metrics:\n",
      "- Avg Latency: 1.4541\n",
      "- Avg Inference Memory Consumption: 1053.8099\n",
      "- Avg Score: 0.4224\n"
     ]
    }
   ],
   "source": [
    "# --- Display W4A16 Quantized Model Metrics ---\n",
    "# Print the overall performance metrics for the W4A16 quantized model.\n",
    "# These metrics include average latency, memory usage, and similarity score.\n",
    "print(\"W4A16 Quantized Model Performance Metrics:\")\n",
    "for key, value in w4a16_model_metrics.items():\n",
    "    # Print each metric with its corresponding value, formatted to 4 decimal places\n",
    "    print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb73cbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[ 41],\n",
       "            [213],\n",
       "            [ 65],\n",
       "            ...,\n",
       "            [ 92],\n",
       "            [ 75],\n",
       "            [135]], device='cuda:0', dtype=torch.uint8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the weight matrix of the query projection (q_proj) \n",
    "# in the self-attention mechanism of the first decoder layer of the quantized model.\n",
    "q_proj_weight = w4a16_model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "# Display the weight tensor\n",
    "q_proj_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e55d27",
   "metadata": {},
   "source": [
    "### For W8A8 Quantized Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71675b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Configure for 8-bit weight + activation quantization (W8A8)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                           # W8\n",
    "    llm_int8_threshold=6.0,                      # Optional: threshold for outlier detection\n",
    "    llm_int8_has_fp16_weight=False,              # Force 8-bit only mode (no fallback to fp16)\n",
    "    llm_int8_enable_fp32_cpu_offload=True,       # Optional: offload to CPU for better memory management\n",
    ")\n",
    "\n",
    "# defining the tokenizer again for clarity\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model with 8-bit weights and 8-bit activations\n",
    "w8a8_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"  # Will use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06eae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model Memory Footprint (MB): 1429.13\n"
     ]
    }
   ],
   "source": [
    "# Memory footprint of the quantized model\n",
    "w8a8_model_memory = get_model_memory_footprint(w8a8_model)\n",
    "print(\"Quantized Model Memory Footprint (MB):\", w8a8_model_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad684a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88379518e1e4f4ab2e853a76c4c457a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Evaluate the W8a8 Quantized Model ---\n",
    "# Running the evaluation on the quantized model with the evaluation dataset\n",
    "w8a8_model_results = evaluate_model(w8a8_model, tokenizer, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12f3bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results:   0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:07<00:00,  6.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate with LLM Judge ---\n",
    "# Use the 'evaluate_with_judge' function to assess the quantized model's performance.\n",
    "w8a8_model_results_df, w4a16_model_metrics = evaluate_with_judge(\n",
    "    w8a8_model_results,  # The results from your quantized model evaluation\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79f688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W8A8 Quantized Model Performance Metrics:\n",
      "- Avg Latency: 5.2888\n",
      "- Avg Inference Memory Consumption: 1489.3576\n",
      "- Avg Score: 0.4245\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation results\n",
    "print(\"W8A8 Quantized Model Performance Metrics:\")\n",
    "for key, value in w4a16_model_metrics.items():\n",
    "    # Print each metric with its corresponding value, formatted to 4 decimal places\n",
    "    print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c025fcd",
   "metadata": {},
   "source": [
    "# AWQ (Activation Weight Quantization)\n",
    "\n",
    "**AWQ** stands for **Activation-aware Weight Quantization**.\n",
    "It is a **post-training quantization (PTQ)** method that:\n",
    "\n",
    "* Quantizes **only the weights** (usually to **int4** or **int8**)\n",
    "* **Leaves activations in full precision** (e.g., float16 or bfloat16)\n",
    "* Is designed specifically for **LLMs** (e.g., LLaMA, Mistral, etc.)\n",
    "* Supports **per-channel quantization** and **outlier-aware scaling**\n",
    "\n",
    "#### So AWQ is a form of **Weight-only quantization (WOQ)** — typically **W4A16** or **W8A16** depending on the config.m,m\n",
    "\n",
    "## AWQ = W4A16 or W8A16?\n",
    "\n",
    "| Bit Precision | Weights | Activations | AWQ Supports        |\n",
    "| ------------- | ------- | ----------- | ------------------- |\n",
    "| W4A16         | 4-bit   | 16-bit      | Yes (default)     |\n",
    "| W8A16         | 8-bit   | 16-bit      | Yes (less common) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ-style W4A16 Configuration\n",
    "awq_w4a16_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,          # Enables double quantization for better accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",               # Normal Float 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16    # Activations remain in bfloat16 (W4A16)\n",
    ")\n",
    "\n",
    "# AWQ-style W8A16 Configuration  \n",
    "awq_w8a16_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                       # 8-bit weights\n",
    "    llm_int8_threshold=6.0,                  # Threshold for outlier detection\n",
    "    llm_int8_has_fp16_weight=False,          # Force 8-bit weights (no fp16 fallback)\n",
    "    llm_int8_enable_fp32_cpu_offload=False   # Keep activations in fp16/bf16 (W8A16)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8ac4c",
   "metadata": {},
   "source": [
    "# KV Cache\n",
    "\n",
    "During **autoregressive decoding** (e.g., generating text token by token), models store previously computed:\n",
    "\n",
    "* **K (Key)**\n",
    "* **V (Value)**\n",
    "\n",
    "…vectors in memory so they don’t need to be recomputed every time. This is called the **Key-Value (KV) cache**.\n",
    "\n",
    "But:\n",
    "\n",
    "* These KV tensors are usually stored in **fp16**/**bf16**/**fp32** by default.\n",
    "* For long sequences and large batches, **KV cache becomes a huge memory bottleneck**.\n",
    "\n",
    "It refers to quantizing the **Key** and **Value** tensors to lower precision **only for caching**, e.g.:\n",
    "\n",
    "* `int8` (8-bit)\n",
    "* `int4` (4-bit)\n",
    "\n",
    "While:\n",
    "\n",
    "* The rest of the model stays in higher precision (e.g., bfloat16 or float16)\n",
    "* Attention scores are still computed accurately\n",
    "\n",
    "Cache can be implemented in various ways, such as:\n",
    "- **use_cache** (bool, *optional*, defaults to `True`)  \n",
    "  Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "\n",
    "- **cache_implementation** (str, *optional*, defaults to `None`)  \n",
    "  Name of the cache class that will be instantiated in `generate`, for faster decoding.  \n",
    "  Possible values are:\n",
    "  - [dynamic](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.DynamicCache): Dynamically grows key/value cache during generation (default).\n",
    "  - [static](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.StaticCache): Preallocates fixed-size cache for faster JIT-compiled decoding.\n",
    "  - [offloaded_static](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.OffloadedStaticCache): Like static, but offloads unused layers to CPU to save GPU memory.\n",
    "  - [sliding_window](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.SlidingWindowCache): Keeps only the most recent tokens in memory for efficient long-gen.\n",
    "  - [hybrid](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.HybridCache): Combines sliding window and static strategies for context and speed.\n",
    "  - [mamba](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.MambaCache): Specialized cache for models like Mamba or Gemma2.\n",
    "  - [quantized](https://huggingface.co/docs/transformers/v4.53.0/en/internal/generation_utils#transformers.QuantizedCache): Uses low-bit precision (int2/4/8) to reduce memory usage with minimal quality drop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1feab",
   "metadata": {},
   "source": [
    "We will implement and evaluate KV Caching on the original base model and then on the quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "100800f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mcache_implementation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.generation_config.cache_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73d3eb",
   "metadata": {},
   "source": [
    "### For Base Model Evaluation with KV Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef82603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d82d40e5b3e4d23bfbf75076b129279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating original model using KV Caching\n",
    "base_model_kv_results = evaluate_model(\n",
    "    original_model,\n",
    "    tokenizer,\n",
    "    eval_data,\n",
    "    use_cache=True,  # Enable KV caching\n",
    "    cache_implementation=\"quantized\",  # Use Quantized KV caching\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure padding token is set (To remove warnings about pad_token_id not being set )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a90aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:09<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model with KV Caching Performance Metrics:\n",
      "- Avg Latency: 6.2055\n",
      "- Avg Inference Memory Consumption: 1241.7709\n",
      "- Avg Score: 0.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate with LLM Judge ---\n",
    "# Use the 'evaluate_with_judge' function to assess the model's performance with KV caching.\n",
    "base_model_kv_results_df, base_model_kv_metrics = evaluate_with_judge(\n",
    "    base_model_kv_results,  # The results from your model evaluation with KV cache\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")\n",
    "\n",
    "# Printing the evaluation results\n",
    "print(\"Base Model with KV Caching Performance Metrics:\")\n",
    "for key, value in base_model_kv_metrics.items():\n",
    "    # Print each metric with its corresponding value, formatted to 4 decimal places\n",
    "    print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2927e17",
   "metadata": {},
   "source": [
    "### For W4A16 Quantized Model with KV Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07f8dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635a7a8c945d4e198a1527dc60eb36a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:08<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4A16 Model with KV Caching Performance Metrics:\n",
      "- Avg Latency: 1.1640\n",
      "- Avg Inference Memory Consumption: 1053.6658\n",
      "- Avg Score: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating W4A16 Quantized Model using KV Caching\n",
    "w4a16_model_kv_results = evaluate_model(\n",
    "    w4a16_model,\n",
    "    tokenizer,\n",
    "    eval_data,\n",
    "    use_cache=True,  # Enable KV caching\n",
    "    cache_implementation=\"quantized\",  # Use Quantized KV caching\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure padding token is set\n",
    ")\n",
    "\n",
    "# --- Evaluate with LLM Judge ---\n",
    "w4a16_model_kv_results_df, w4a16_model_kv_metrics = evaluate_with_judge(\n",
    "    w4a16_model_kv_results,  # The results from your model evaluation with KV cache\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")\n",
    "\n",
    "# Printing the evaluation results\n",
    "print(\"W4A16 Model with KV Caching Performance Metrics:\")\n",
    "for key, value in w4a16_model_kv_metrics.items():\n",
    "    # Print each metric with its corresponding value, formatted to 4 decimal places\n",
    "    print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0da389",
   "metadata": {},
   "source": [
    "# Model Performance Comparison (Weight Only Quantization + KV Caching)\n",
    "\n",
    "There is a reason why companies like Together.ai or Nebius AI have implemented these optimization techniques, especially the KV Caching + Weight-Only Quantization, to reduce the cost of hosting an LLM.\n",
    "\n",
    "This table summarizes the key performance metrics for the `meta-llama/Llama-3.2-1B` model under different optimization techniques.\n",
    "\n",
    "| Model Configuration | Model Memory (MB) | Avg. Latency (s) | Avg. Peak Memory (MB) | Avg. LLM Judge Score |\n",
    "| :------------------ | :---------------- | :--------------- | :-------------------- | :------------------- |\n",
    "| **Base Model (bf16)** | 2357.13 | 4.46 | 1240.21 | 0.39 |\n",
    "| **W4A16 Quantized** | **965.13** | 1.45 | **1053.81** | 0.42 |\n",
    "| **W8A8 Quantized** | 1429.13 | 5.29 | 1489.36 | 0.42 |\n",
    "| **Base + KV Cache** | 2357.13 | 6.21 | 1241.77 | **0.45** |\n",
    "| **W4A16 + KV Cache** | **965.13** | **1.16** | 1053.67 | 0.42 |\n",
    "\n",
    "Overall, the results show that the W4A16 quantized model with KV caching provides a good balance of speed, memory efficiency, and response quality.\n",
    "\n",
    "# Other Observations:\n",
    "\n",
    "*   **Best Latency:** The combination of **W4A16 Quantization and KV Caching** provides the lowest latency (`1.16s`), making it the fastest for inference.\n",
    "*   **Lowest Memory Usage:** **W4A16 quantization** drastically reduces both the static model memory footprint (`965.13 MB`) and the peak memory used during inference.\n",
    "*   **Highest Accuracy:** The **Base Model with KV Caching** achieved the highest LLM Judge score (`0.45`), suggesting that while it was slower, its response quality was slightly better.\n",
    "*   **Trade-offs:** The results highlight a clear trade-off between speed/memory and response quality. W4A16 offers a great balance, significantly improving performance with a minimal impact on the evaluation score. The W8A8 configuration was slower and used more memory than the base model in this specific test, which might be due to implementation overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209198b",
   "metadata": {},
   "source": [
    "# SDPA Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31f25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flash_attention_3',\n",
       " 'flash_attention_2',\n",
       " 'flex_attention',\n",
       " 'paged_attention',\n",
       " 'sdpa',\n",
       " 'sdpa_paged',\n",
       " 'eager_paged']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see which attention implementations are available in your transformers installation,\n",
    "# you can inspect the `ALL_ATTENTION_FUNCTIONS` dictionary.\n",
    "# This is useful for knowing what strings you can pass to the `attn_implementation` argument.\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "\n",
    "list(ALL_ATTENTION_FUNCTIONS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aebb8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define quantization config: 4-bit weights (W4A16)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,          # optional: improves accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",               # \"nf4\" (normal float 4-bit) or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16    # use bfloat16 for activations (W4A16)\n",
    ")\n",
    "\n",
    "# SDPA (Sparse Distributed Parallel Attention) is a memory-efficient attention implementation.\n",
    "w4a16_model_sdpa = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # use \"auto\" for best device placement (GPU/CPU)\n",
    "    attn_implementation=\"sdpa\",  # Use memory-efficient SDPA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b35ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDPA_PAGED (Sparse Distributed Parallel Attention with Paged Memory) is another memory-efficient attention implementation.\n",
    "w4a16_model_sdpa_paged = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # use \"auto\" for best device placement (GPU/CPU)\n",
    "    attn_implementation=\"sdpa_paged\",  # Use memory-efficient SDPA_PAGED,\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the W4A16 SDPA Model and W4A16 SDPA Paged Model\n",
    "w4a16_model_sdpa_results = evaluate_model(w4a16_model_sdpa, tokenizer, eval_data)\n",
    "w4a16_model_sdpa_paged_results = evaluate_model(w4a16_model_sdpa_paged, tokenizer, eval_data)\n",
    "\n",
    "# --- Evaluate both results with LLM Judge ---\n",
    "w4a16_model_sdpa_results_df, w4a16_model_sdpa_metrics = evaluate_with_judge(\n",
    "    w4a16_model_sdpa_results,  # The results from your SDPA model evaluation\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")\n",
    "w4a16_model_sdpa_paged_results_df, w4a16_model_sdpa_paged_metrics = evaluate_with_judge(\n",
    "    w4a16_model_sdpa_paged_results,  # The results from your SDPA Paged model evaluation\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0eb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "965.13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_memory_footprint(w4a16_model_sdpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ae253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the evaluation on the model with the evaluation dataset\n",
    "w4a16_model_sdpa_results = evaluate_model(w4a16_model_sdpa, tokenizer, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c61136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:09<00:00,  5.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate with LLM Judge ---\n",
    "# Use the 'evaluate_with_judge' function to assess the quantized model's performance.\n",
    "w4a16_model_sdpa_results_df, w4a16_model_sdpa_metrics = evaluate_with_judge(\n",
    "    base_model_results,  # The results from your quantized model evaluation\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d128f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_latency': np.float64(1.3038163265306122),\n",
       " 'avg_inference_memory_consumption': np.float64(1053.8089591836736),\n",
       " 'avg_score': np.float64(0.3918367346938776)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w4a16_model_sdpa_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2d77b",
   "metadata": {},
   "source": [
    "# SDPA Performance Comparison\n",
    "\n",
    "This table summarizes the performance of the W4A16 quantized model with different SDPA (Scalable Dot-Product Attention) implementations.\n",
    "\n",
    "| Model Configuration      | Avg. Latency (s) | Avg. Peak Memory (MB) | Avg. LLM Judge Score |\n",
    "| :----------------------- | :--------------- | :-------------------- | :------------------- |\n",
    "| **W4A16 + SDPA**         | 1.103            | 1003.81               | 0.421                |\n",
    "| **W4A16 + SDPA Paged**   | 1.303            | 1041.80               | 0.391                |\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "*   **Latency:** The standard `sdpa` implementation is slightly faster than `sdpa_paged`.\n",
    "*   **Memory:** The standard `sdpa` implementation also consumes less peak memory during inference.\n",
    "*   **Accuracy:** The `sdpa` implementation achieves a higher LLM Judge score, indicating better response quality in this evaluation.\n",
    "\n",
    "Overall, for this specific model and task, the standard `sdpa` attention implementation provides a better balance of performance and accuracy compared to the `sdpa_paged` version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572bfb9",
   "metadata": {},
   "source": [
    "# SDPA + KV Cache + Weight Only Quantization (W4A16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de6e760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cache strategies:\n",
      "- Cache\n",
      "- CacheConfig\n",
      "- DynamicCache\n",
      "- EncoderDecoderCache\n",
      "- HQQQuantizedCache\n",
      "- HybridCache\n",
      "- HybridChunkedCache\n",
      "- MambaCache\n",
      "- OffloadedCache\n",
      "- OffloadedHybridCache\n",
      "- OffloadedStaticCache\n",
      "- QuantizedCache\n",
      "- QuantizedCacheConfig\n",
      "- QuantoQuantizedCache\n",
      "- SinkCache\n",
      "- SlidingWindowCache\n",
      "- StaticCache\n",
      "- StaticCacheConfig\n"
     ]
    }
   ],
   "source": [
    "import transformers.cache_utils as cache_utils\n",
    "\n",
    "# List all cache classes available in the module\n",
    "cache_classes = [cls for cls in dir(cache_utils) if \"Cache\" in cls and not cls.startswith(\"_\")]\n",
    "print(\"Available cache strategies:\")\n",
    "for cls in cache_classes:\n",
    "    print(f\"- {cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d836fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of KV cache strategies to evaluate\n",
    "# These are different implementations for caching key-value pairs during text generation\n",
    "kv_cache_strategies = [\n",
    "    \"dynamic\", # DynamicCache: dynamically manages cache size based on input length\n",
    "    \"static\", # StaticCache: uses a fixed-size cache for all inputs\n",
    "    # \"sliding_window\", # SlidingWindowCache: maintains a sliding window of the most recent tokens\n",
    "    \"quantized\", # QuantizedCache: uses quantization techniques to reduce memory usage\n",
    "    # The following might require specific model architectures or additional setup\n",
    "    # \"offloaded_static\",\n",
    "    # \"hybrid\",\n",
    "    # \"mamba\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a2db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with cache strategy: dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668a645f281248b2ab72424d408a36a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:07<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic Performance Metrics:\n",
      "- Avg Latency: 0.8046\n",
      "- Avg Inference Memory Consumption: 1056.3977\n",
      "- Avg Score: 0.4327\n",
      "Evaluating with cache strategy: static\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c6e73b0cb241e8b732231f24894b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:09<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static Performance Metrics:\n",
      "- Avg Latency: 4.0182\n",
      "- Avg Inference Memory Consumption: 1056.1599\n",
      "- Avg Score: 0.4224\n",
      "Evaluating with cache strategy: quantized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e7dc90a18b49f7ba55e86b01611d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:07<00:00,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized Performance Metrics:\n",
      "- Avg Latency: 5.4194\n",
      "- Avg Inference Memory Consumption: 1056.2500\n",
      "- Avg Score: 0.4143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform kv cache evaluation on w4a16_model_sdpa (W4A16 SDPA model)\n",
    "for cache_strategy in kv_cache_strategies:\n",
    "    print(f\"Evaluating with cache strategy: {cache_strategy}\")\n",
    "    \n",
    "    # Evaluate the model with the specified cache strategy\n",
    "    w4a16_model_sdpa_kv_results = evaluate_model(\n",
    "        w4a16_model_sdpa,\n",
    "        tokenizer,\n",
    "        eval_data,\n",
    "        use_cache=True,  # Enable KV caching\n",
    "        cache_implementation=cache_strategy,  # Use specified cache strategy\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Ensure padding token is set\n",
    "    )\n",
    "    \n",
    "    # --- Evaluate with LLM Judge ---\n",
    "    w4a16_model_sdpa_kv_results_df, w4a16_model_sdpa_kv_metrics = evaluate_with_judge(\n",
    "        w4a16_model_sdpa_kv_results,  # The results from your model evaluation with KV cache\n",
    "        api_key=LLM_API_KEY,  # API key for the judging service\n",
    "        model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    "    )\n",
    "    \n",
    "    # Print the evaluation results for the current cache strategy\n",
    "    print(f\"{cache_strategy} Performance Metrics:\")\n",
    "    for key, value in w4a16_model_sdpa_kv_metrics.items():\n",
    "        print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec131c",
   "metadata": {},
   "source": [
    "# Multi Round Conversation Optimization (Remembering past interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286c397",
   "metadata": {},
   "source": [
    "Let's look at this example:\n",
    "\n",
    "### Example of Multi-Chat Conversation\n",
    "\n",
    "> **User:** What's the capital of Japan?\n",
    "> **Assistant:** Tokyo is the capital of Japan.\n",
    "> **User:** And what about China?\n",
    "\n",
    "The model generates replies one at a time using **auto-regressive decoding**.\n",
    "\n",
    "#### First Reply:\n",
    "- The cache is empty.\n",
    "- The model reads: `\"User: What's the capital of Japan?\"`\n",
    "- It generates: `\"Tokyo is the capital of Japan.\"`\n",
    "- While doing this, it stores useful data (keys and values) in memory.\n",
    "\n",
    "#### Second Reply:\n",
    "- The model sees this full history: `\"User: What's the capital of Japan? \\n Assistant: Tokyo is the capital of Japan. \\n User: And what about China?\"`\n",
    "- But thanks to the cache, it doesn't have to reprocess the whole thing.\n",
    "- It reuses the stored data and only processes: `\"User: And what about China?\"`\n",
    "- Then it generates: `\"Beijing is the capital of China.\"`\n",
    "\n",
    "Two important things should be noted here:\n",
    "- **Context matters:** When the user asks \"And what about China?\", the model understands it's about capitals because of the earlier question.\n",
    "- **Cache saves work:** The key-value cache lets the model reuse earlier parts of the chat without re-reading everything, making it faster and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5586f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>What is the capital of Japan? 2019\n",
      "What is the capital of Japan?\n",
      "A. Osaka\n",
      "B. Tokyo\n",
      "C. Fukuoka\n",
      "D. Nagoya\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "question = \"What is the capital of Japan?\"\n",
    "model_inputs = tokenizer(question, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "# Generate the first answer\n",
    "output = w4a16_model_sdpa.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=30,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,  # Enable KV caching\n",
    "    cache_implementation=\"dynamic\",  # Use best cache strategy for ourcase\n",
    ")\n",
    "\n",
    "# Detokenizing the answer\n",
    "answer1 = tokenizer.batch_decode(output.sequences)[0]\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df046b",
   "metadata": {},
   "source": [
    "But then we also need to store the decoded values for the future conversation, so …."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db5dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question, using past_key_values for efficiency\n",
    "follow_up = \"What about China?\"\n",
    "model_inputs = tokenizer(answer1 + \"\\n\" + follow_up, return_tensors='pt').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc51cbb",
   "metadata": {},
   "source": [
    "The key parameter to keep in mind is `past_key_values`, which prevents the model from recalculating the KV cache for the parts of the conversation that have already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b74d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What about China? 2019\n",
      "What about China?\n",
      "A. Beijing\n",
      "B. Guangzhou\n",
      "C. Shanghai\n",
      "D. Hong Kong\n",
      "E. Shenzhen\n"
     ]
    }
   ],
   "source": [
    "output = w4a16_model_sdpa.generate(\n",
    "    **model_inputs,\n",
    "    past_key_values=output.past_key_values, # It let's model avoid recalc of KV Cache\n",
    "    max_new_tokens=30,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,  # Enable KV caching\n",
    "    # cache_implementation=\"dynamic\",  # Use best cache strategy for ourcase\n",
    ")\n",
    "answer2 = tokenizer.batch_decode(output.sequences)[0][len(answer1 + \"\\n\" + follow_up):]\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eeb0c1",
   "metadata": {},
   "source": [
    "A better system would be to use the `past_key_values` parameter in the `generate` method, which allows the model to answer follow-up questions correctly without hallucinating or forgetting previous context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3003a",
   "metadata": {},
   "source": [
    "# Prompt Lookup Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cdb3e",
   "metadata": {},
   "source": [
    "In many cases your hosted LLM needs original tokens that are already available in context of LLM these tasks are like summarization, Text simplification or QA and so. and smaller llms are a part of bigger architecture which handles such tasks to reduce the overall tasks also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Name the capital city of Japan? Answer: Tokyo\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "# Define the prompt for the model\n",
    "question = \"Name the capital city of Japan?\"\n",
    "# Tokenize the input question and move it to the GPU for processing\n",
    "model_inputs = tokenizer(question, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "\n",
    "# Generate a response using the model with prompt lookup decoding\n",
    "output = w4a16_model_sdpa.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=5, # Limit the generation to 5 new tokens\n",
    "    return_dict_in_generate=True, # Return a detailed output object\n",
    "    use_cache=True,  # Enable KV caching to speed up decoding\n",
    "    cache_implementation=\"dynamic\",  # Use the dynamic cache implementation for efficiency\n",
    "    \n",
    "    # Specify the number of prompt tokens to use for lookup decoding.\n",
    "    # This speeds up processing by matching the initial tokens of the prompt\n",
    "    # against a pre-computed table, avoiding redundant computations.\n",
    "    prompt_lookup_num_tokens=3,\n",
    "\n",
    ")\n",
    "# Decode the generated tokens back into a human-readable string\n",
    "answer = tokenizer.batch_decode(output.sequences)[0]\n",
    "# Print the final answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bfe5e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d56175d1534c549c9086c81c184070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating model:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performing evaluation with prompt lookup decoding with kv cache\n",
    "prompt_lookup_results = evaluate_model(\n",
    "    w4a16_model_sdpa,  # The model to evaluate\n",
    "    tokenizer,         # The tokenizer for encoding input text\n",
    "    eval_data,         # The evaluation dataset\n",
    "    use_cache=True,    # Enable KV caching\n",
    "    cache_implementation=\"dynamic\",  # Use the dynamic cache implementation\n",
    "    prompt_lookup_num_tokens=10,  # Use 10 tokens for prompt lookup decoding\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure padding token is set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b034f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring results: 100%|██████████| 49/49 [00:08<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Lookup Decoding Performance Metrics:\n",
      "- Avg Latency: 1.2086\n",
      "- Avg Inference Memory Consumption: 1056.7745\n",
      "- Avg Score: 0.3837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with LLM Judge\n",
    "prompt_lookup_results_df, prompt_lookup_metrics = evaluate_with_judge(\n",
    "    prompt_lookup_results,  # The results from your model evaluation with prompt lookup decoding\n",
    "    api_key=LLM_API_KEY,  # API key for the judging service\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\"  # Name of the model being evaluated\n",
    ")\n",
    "\n",
    "# Print the evaluation results for prompt lookup decoding\n",
    "print(\"Prompt Lookup Decoding Performance Metrics:\")\n",
    "for key, value in prompt_lookup_metrics.items():\n",
    "    # Print each metric with its corresponding value, formatted to 4 decimal places\n",
    "    print(f\"- {key.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_host_llm_guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
